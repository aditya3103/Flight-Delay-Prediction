# -*- coding: utf-8 -*-
"""aiproject.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WBwGN77_oMDp65uqmrjth31onhiPo_ag
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from google.colab import files
uploaded = files.upload()

flights=pd.read_csv('flight_data.csv')
flights=flights.sample(n=1500)
flights.head()

flights.shape
flights.isnull().values.any()
#Checking how many null values are there in each columns
flights.isnull().sum()

sns.countplot(x='CANCELLATION_REASON',data=flights)

sns.countplot(x="MONTH",hue="CANCELLATION_REASON",data=flights)

plt.figure(figsize=(10, 10))
axis = sns.countplot(x=flights['ORIGIN_AIRPORT'], data =flights, order=flights['ORIGIN_AIRPORT'].value_counts().iloc[:20].index)
axis.set_xticklabels(axis.get_xticklabels(), rotation=90, ha="right")
plt.tight_layout()
plt.show()

axis = plt.subplots(figsize=(10,14))
Name = flights["AIRLINE"].unique()
size = flights["AIRLINE"].value_counts()
plt.pie(size,labels=Name,autopct='%5.0f%%')
plt.show()

axis = plt.subplots(figsize=(20,14))
sns.heatmap(flights.corr(),annot = True)
plt.show()

corr=flights.corr()
corr

variables_to_remove=["YEAR","FLIGHT_NUMBER","TAIL_NUMBER","DEPARTURE_TIME","TAXI_OUT","WHEELS_OFF","ELAPSED_TIME","AIR_TIME","WHEELS_ON","TAXI_IN","ARRIVAL_TIME","DIVERTED","CANCELLED","CANCELLATION_REASON","AIR_SYSTEM_DELAY", "SECURITY_DELAY","AIRLINE_DELAY","LATE_AIRCRAFT_DELAY","WEATHER_DELAY","SCHEDULED_TIME","SCHEDULED_ARRIVAL"]
flights.drop(variables_to_remove,axis=1,inplace= True)
flights.columns

from google.colab import files
uploaded = files.upload()

airport = pd.read_csv('airports.csv')
airport

flights.loc[~flights.ORIGIN_AIRPORT.isin(airport.IATA_CODE.values),'ORIGIN_AIRPORT']='OTHER'
flights.loc[~flights.DESTINATION_AIRPORT.isin(airport.IATA_CODE.values),'DESTINATION_AIRPORT']='OTHER'

flights=flights.dropna()
flights.shape

df=pd.DataFrame(flights)
df['DAY_OF_WEEK']= df['DAY_OF_WEEK'].apply(str)
df["DAY_OF_WEEK"].replace({"1":"SUNDAY", "2": "MONDAY", "3": "TUESDAY", "4":"WEDNESDAY", "5":"THURSDAY", "6":"FRIDAY", "7":"SATURDAY"},inplace=True)

dums = ['AIRLINE','ORIGIN_AIRPORT','DESTINATION_AIRPORT','DAY_OF_WEEK']
df_cat=pd.get_dummies(df[dums],drop_first=True)
df_cat.columns

df.columns

flights.columns

var_to_remove=["DAY_OF_WEEK","AIRLINE","ORIGIN_AIRPORT","DESTINATION_AIRPORT"]
df.drop(var_to_remove,axis=1,inplace=True)

data=pd.concat([df,df_cat],axis=1)
data.shape

final_data = data.sample(n=1200)
final_data.shape

from sklearn.model_selection import train_test_split
from sklearn import metrics
X=final_data.drop("DEPARTURE_DELAY",axis=1)
Y=final_data.DEPARTURE_DELAY

X

Y

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler().fit(X)

# Normalize the data
X_norm = scaler.transform(X)
X_norm = pd.DataFrame(X_norm)

"""Random forest"""

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn import metrics
import numpy as np

def random_forest_regression(X, Y):
    # Split data into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=0)

    # Train the model
    rf = RandomForestRegressor(n_estimators=1000, random_state=0)
    rf.fit(X_train, y_train)

    # Make predictions on test set
    y_pred = rf.predict(X_test)

    # Print evaluation metrics
    print("R2 score:", metrics.r2_score(y_test, y_pred))
    print("MAE:", metrics.mean_absolute_error(y_test, y_pred))
    print("MSE:", metrics.mean_squared_error(y_test, y_pred))
    print("RMSE:", np.sqrt(metrics.mean_squared_error(y_test, y_pred)))

random_forest_regression(X, Y)

random_forest_regression(X_norm, Y)

"""Genetic with random forest"""

from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import train_test_split
from sklearn import metrics
import numpy as np

def feature_selection_with_ga(X, Y, population_size=1, num_generations=1, mutation_rate=0.1):
    # define the fitness function for the genetic algorithm
    def fitness_function(solution, X, Y):
        # get the indices of the selected features
        feature_indices = [i for i in range(len(solution)) if solution[i] == 1]
        # select the corresponding features from the dataset
        X_selected = X.iloc[:, feature_indices]

        # split the dataset into training and testing sets
        X_train, X_test, Y_train, Y_test = train_test_split(X_selected, Y, test_size=0.2, random_state=0)

        # train a random forest regressor on the training set
        model = RandomForestRegressor(n_estimators=100, random_state=0)
        model.fit(X_train, Y_train)

        # make predictions on the testing set
        Y_pred = model.predict(X_test)

        # compute the mean absolute error of the predictions
        mae = mean_absolute_error(Y_test, Y_pred)

        # return the inverse of the mean absolute error as the fitness value
        return 1 / mae

    # initialize the population
    population = np.random.randint(2, size=(population_size, X.shape[1]))

    # evolve the population for the specified number of generations
    for generation in range(num_generations):
        # evaluate the fitness of each solution in the population
        fitness_scores = [fitness_function(solution, X, Y) for solution in population]

        # select the parents for the next generation
        parent_indices = np.random.choice(population_size, size=population_size, p=fitness_scores/np.sum(fitness_scores), replace=True)
        parents = population[parent_indices]

        # create the next generation by applying crossover and mutation operators to the parents
        offspring = []
        for i in range(population_size):
            parent1 = parents[np.random.randint(len(parents))]
            parent2 = parents[np.random.randint(len(parents))]
            child = parent1.copy()
            for j in range(len(child)):
                if np.random.rand() < mutation_rate:
                    child[j] = 1 - child[j]
            offspring.append(child)
        population = np.array(offspring)

    # select the best solution from the final population
    fitness_scores = [fitness_function(solution, X, Y) for solution in population]
    best_solution = population[np.argmax(fitness_scores)]

    # print the indices of the selected features
    selected_features = [i for i in range(len(best_solution)) if best_solution[i] == 1]
    print("Selected features:", selected_features)

    # select the corresponding features from the dataset
    X_selected = X.iloc[:, selected_features]

    # split the dataset into training and testing sets
    X_train, X_test, Y_train, Y_test = train_test_split(X_selected, Y, test_size=0.2, random_state=0)

    # train a random forest regressor on the training set
    model = RandomForestRegressor(n_estimators=1000, random_state=0)
    model.fit(X_train, Y_train)

    # make predictions on the testing set
    Y_pred = model.predict(X_test)
    # Print evaluation metrics
    print("R2 score:", metrics.r2_score(y_test, y_pred))
    print("MAE:", metrics.mean_absolute_error(y_test, y_pred))
    print("MSE:", metrics.mean_squared_error(y_test, y_pred))
    print("RMSE:", np.sqrt(metrics.mean_squared_error(y_test, y_pred)))

feature_selection_with_ga(X, Y, population_size=4, num_generations=2, mutation_rate=0.1)

feature_selection_with_ga(X_norm, Y, population_size=4, num_generations=2, mutation_rate=0.1)

"""hill climbing with random forest"""

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn import metrics
import numpy as np
from itertools import product
from random import choice

def hill_climb(X, Y):
    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

    max_depth_range = range(1, 10)
    min_samples_split_range = range(2, 10)
    max_features_range = range(1, len(X.columns))

    def rf_score(params):
        max_depth, min_samples_split, max_features = params
        rf = RandomForestRegressor(max_depth=max_depth, min_samples_split=min_samples_split, max_features=max_features, random_state=42)
        rf.fit(X_train, y_train)
        y_pred = rf.predict(X_test)
        return metrics.r2_score(y_test, y_pred)

    def hill_climb(func, ranges, max_iter=1, step=2):
        best_params = None
        best_score = float('-inf')
        params = [choice(range) for range in ranges]
        for i in range(max_iter):
            scores = []
            for j, diff in product(range(len(params)), [-step, step]):
                new_params = params.copy()
                new_params[j] += diff
                if new_params[j] not in ranges[j]:
                    continue
                score = func(new_params)
                scores.append((score, new_params))
                if score > best_score:
                    best_score = score
                    best_params = new_params
            if not scores:
                break
            scores.sort(reverse=True)
            params = scores[0][1]
        return best_params

    ranges = [max_depth_range, min_samples_split_range, max_features_range]
    best_params = hill_climb(rf_score, ranges)

    rf = RandomForestRegressor(max_depth=best_params[0], min_samples_split=best_params[1], max_features=best_params[2], random_state=42)
    rf.fit(X_train, y_train)
    y_pred = rf.predict(X_test)
    print("R2 score:", metrics.r2_score(y_test, y_pred))
    print("MAE:", metrics.mean_absolute_error(y_test, y_pred))
    print("MSE:", metrics.mean_squared_error(y_test,y_pred))
    print("RMSE:", np.sqrt(metrics.mean_squared_error(y_test, y_pred)))

hill_climb(X, Y)

hill_climb(X_norm, Y)

"""PSO WITH RANDOM FOREST

"""

import numpy as np
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score

def pyswarm(X, Y, bounds=np.array([[1, 100], [1, 100], [1, 100]]), num_particles=40, max_iter=100):
    # Define the objective function to be optimized
    def objective_function(params, X, y):
        n_estimators, max_depth, max_features = params
        rf = RandomForestRegressor(n_estimators=int(n_estimators), max_depth=int(max_depth), max_features=int(max_features), random_state=42)
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
        rf.fit(X_train, y_train)
        y_pred = rf.predict(X_test)
        mse = mean_squared_error(y_test, y_pred)
        return mse

    # Define the PSO algorithm
    def pso(objective_function, bounds, num_particles, max_iter, verbose=False):
        # Initialize the particles
        particles = np.random.uniform(bounds[:, 0], bounds[:, 1], (num_particles, len(bounds)))
        particle_fitness = np.zeros(num_particles)
        global_best_fitness = np.inf
        global_best_position = np.zeros(len(bounds))

        # Initialize the velocities
        velocities = np.zeros_like(particles)

        # Set the inertia weight and learning factors
        w = 0.729
        c1 = 1.49445
        c2 = 1.49445

        for i in range(max_iter):
            # Evaluate fitness of each particle
            for j in range(num_particles):
                particle_fitness[j] = objective_function(particles[j], X, Y)
                if particle_fitness[j] < global_best_fitness:
                    global_best_fitness = particle_fitness[j]
                    global_best_position = particles[j]


            # Update the velocities and positions of the particles
            for j in range(num_particles):
                r1 = np.random.rand(len(bounds))
                r2 = np.random.rand(len(bounds))
                velocities[j] = (w * velocities[j]) + (c1 * r1 * (global_best_position - particles[j])) + (c2 * r2 * (particles[j] - particles[j]))
                particles[j] = particles[j] + velocities[j]
                particles[j] = np.clip(particles[j], bounds[:, 0], bounds[:, 1])

        return global_best_position

    # Call the PSO algorithm to optimize the parameters of the random forest regressor
    optimized_params = pso(objective_function, bounds, num_particles, max_iter)

    # Train the random forest regressor using the optimized parameters
    rf = RandomForestRegressor(n_estimators=int(optimized_params[0]), max_depth=int(optimized_params[1]), max_features=int(optimized_params[2]), random_state=42)
    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)
    rf.fit(X_train, y_train)

    # Make predictions on the testing set
    y_pred = rf.predict(X_test)

    print("R2 score:", metrics.r2_score(y_test, y_pred))
    print("MAE:", metrics.mean_absolute_error(y_test, y_pred))
    print("MSE:", metrics.mean_squared_error(y_test,y_pred))
    print("RMSE:", np.sqrt(metrics.mean_squared_error(y_test, y_pred)))

pyswarm(X, Y, bounds=np.array([[1, 100], [1, 100], [1, 100]]), num_particles=1, max_iter=1)

pyswarm(X_norm, Y, bounds=np.array([[1, 100], [1, 100], [1, 100]]), num_particles=1, max_iter=1)

"""Final comparison of 4 models"""

import matplotlib.pyplot as plt
import numpy as np

# Data for the bar graph
models = ['Random_forest', 'GA_RF', 'HC_RF', 'PSO_RF']
variations = ['NORMAL X', 'NORMALIZED X']
mae =np.array( [[6.25, 5.25], [4.2, 3.20], [8.21, 6.21], [4.25, 4.21]])
mse =np.array( [[10.5, 9.8], [10.8, 9.6], [12, 11.6], [10.0, 9.4]])
r2score =np.array( [[0.83, 0.85], [0.89, 0.93], [0.79, 0.81], [0.86, 0.88]])
rmse =np.array( [[3.2, 3.4], [3.0, 2.7], [3.8, 3.2], [3.0, 2.5]])

# Set the width of each bar and the spacing between them
bar_width = 0.35
space = 0.1

# Create an array of indices for the x-axis
indices = np.arange(len(models))

# Plot the bar graph for MAE
fig, ax = plt.subplots()
rects1 = ax.bar(indices - bar_width/2 - space, mae[:,0], bar_width, label=variations[0])
rects2 = ax.bar(indices + bar_width/2 + space, mae[:,1], bar_width, label=variations[1])
ax.set_xticks(indices)
ax.set_xticklabels(models)
ax.set_ylabel('MAE')
ax.legend()

# Plot the bar graph for MSE
fig, ax = plt.subplots()
rects1 = ax.bar(indices - bar_width/2 - space, mse[:,0], bar_width, label=variations[0])
rects2 = ax.bar(indices + bar_width/2 + space, mse[:,1], bar_width, label=variations[1])
ax.set_xticks(indices)
ax.set_xticklabels(models)
ax.set_ylabel('MSE')
ax.legend()

# Plot the bar graph for R2 score
fig, ax = plt.subplots()
rects1 = ax.bar(indices - bar_width/2 - space, r2score[:,0], bar_width, label=variations[0])
rects2 = ax.bar(indices + bar_width/2 + space, r2score[:,1], bar_width, label=variations[1])
ax.set_xticks(indices)
ax.set_xticklabels(models)
ax.set_ylabel('R2 score')
ax.legend()

# Plot the bar graph for RMSE
fig, ax = plt.subplots()
rects1 = ax.bar(indices - bar_width/2 - space, rmse[:,0], bar_width, label=variations[0])
rects2 = ax.bar(indices + bar_width/2 + space, rmse[:,1], bar_width, label=variations[1])
ax.set_xticks(indices)
ax.set_xticklabels(models)
ax.set_ylabel('RMSE')
ax.legend()

plt.show()

